# 데이터마이닝

- 데이터 마이닝 프로세스: 정의 - 준비 - 가공 - 적용 - 검증 (정준가적검?)
- 데이터 분할: 훈련, 검정, 평가 -> 50:30:20

## 데이터 분할을 통한 과적합, 과소적합 검증

1. 홀드아웃: 전체 데이터를 랜덤하게 추출하여 학습 데이터와 테스트 데이터로 분리하는 방식: Training & Test

   - 검증용 데이터로 하이퍼파라미터를 튜닝하는 단계 생략, validation X

   - 일반적으로 학습 데이터 80%, 테스트 데이터 20%로 설정
   - 데이터 수가 적을 경우 각 데이터셋이 전체 데이터를 대표하지 못할 가능성이 큼

2. K-Fold 교차 검증 : k개 집단 구분, k-1개 학습 데이터셋, 나머지 1개 테스트 데이터셋.
3. LOOCV : 데이터셋을 데이터 크기인 N개 집단으로 나눠서, N-1개를 학습 데이터, 나머지 1개를 테스트 데이터로 사용. (100개 데이터면, 99:1로 100번 훈련), K=N인 K-Fold
4. 붓스트랩 : 원본 데이터 크기만큼 복원추출 수행, 한 번도 뽑히지 않을 확률 약 36.8% <- OOB(Out of Bag)
5. 계층별 k-Fold 교차 검증 : 각 폴드가 가지는 레이블의 분포가 유사하도록 폴드를 추출(참 1000, 거짓 10 일때, 이 불균형을 고려한다는 뜻)
6. 오버샘플링 & 언더샘플링 : 데이터 셋 불균형 해결을 위해, 적은 데이터를 확장(오버샘플링), 많은 데이터를 축소(언더샘플링)

# 분류 분석

## 로지스틱 회귀 분석

- x:연속형, y:범주형 (x가 범주형이더라도, x를 더미변수로 변환하여 가능함)
- Odds(승산): 성공확률/실패확룔 = p/(1-p)
- 로짓변환 : log(Odds) -> y: -inf~inf 의 범위
- 시그모이드 함수 : 로짓 함수의 역함수 (y: 0~1범위)

## 의사결정나무

- 회귀트리와 분류트리로 구분됨

1. 뿌리마디: 최상위마디
2. 자식마디: 하나의 마디로부터 나온 2개 이상의 하위 마디
3. 부모마디: 모든 자식마디의 바로 상위의 마디
4. 끝마디: 자식마디가 없는 최하위 마디
5. 중간마디: 부모마디와 자식마디를 모두 보유한 마디
6. 가지: 부모마디와 자식마디를 연결하는 연결선
7. 깊이: 뿌리부터 특정 마디까지의 단계 수, 뿌리를 0으로 놓고..
8. 트리의 깊이: 뿌리부터 끝 마디 까지의 단계 수. 끝 마디의 깊이가 7이라면, 트리의 깊이가 7

- 분리 기준: 불순도를 사용 (불순도: 자료들의 범주가 한 그룹 안에 얼마나 섞여있는지, 다양한 범주의 데이터로 구성되어 있으면 불순도 값이 큼)
- 정지규칙

  1. 뿌리마디로부터 일정 깊이에 도달할 시 <- 보통 최대 깊이를 정해줌
  2. 불순도의 감소량이 매우 작아 분리의 의미가 없을 시
  3. 마디에 속하는 자료가 일정 수 이하일 시
  4. 모든 자료들이 하나의 그룹에 속할 시

- 가지치기를 통해 과적합 방지
- 검증용 데이터, 이익 도표 등을 통해 평가

### 분리 기준

- 종속변수가 이산형일 경우 : 분류 트리 사용, 분리 기준으로 카이제곱 검정, 지니 지수, 엔트로피 지수(카지엔) , 가장 최소화하는 방향으로.
  1. 카이제곱 : CHAID, p값 최소화
  2. 지니 지수 : CART, 지니 지수 최소화
  3. 엔트로피 지수 : C4.5, 엔트로피 지수 최소화
     - 카이제곱, 지니 지수, 엔트로피 지수는 함수식 외우기
- 종속변수가 연속형일 경우 : 회귀 트리 사용, 분리 기준으로 F-통계량, 분산의 감소량 등 사용
  1. F-통계량 : CHAID (F통계량이 커지는!!, p-value가 작아지는 방향으로)
  2. 분산감소량 : CART (분산의 감소량이 커지는 방향으로)

### 의사결정나무의 장점

1. 모델이 직관적이고 해석이 용이
2. 데이터 정규화 및 단위 변환 불필요
3. 데이터의 선형성, 정규성 등의 가정이 불필요
4. 이산형, 연속형 모두에 적용 가능
5. 전처리 작업 비교적 쉬움
6. 결측치, 이상값에 민감하지 않음

### 의사결정나무의 단점

1. 독립변수 간 중요도 판단 어려움
2. 분류 경계선 근처 자료에 대해 오차가 큼
3. 과적합 발생 가능성이 높음
4. 연속적인 경계를 잘 표현하지 못함
5. 작은 변화에도 트리 구조가 크게 바뀔 수 있음 (불안정성) = 분산이 큼

## 앙상블 분석

- 배깅, 부스팅, 랜덤포레스트 가 있다.

### 배깅

- 여러개의 붓스트랩을 여러 독립적인 모델(분류기)로 학습하여 집계하는 알고리즘, 보팅을 통해 최종 결과값 선정.
  - 모집단의 특성이 잘 반영되는, 분산이 작고 좋은 예측력을 보여줌
  - 붓스트랩: 원본 데이터로부터 랜덤복원추출한 '원본데이터와 같은 크기'의 데이터

### 부스팅

- 여러 개의 모형을 구축한다는 점에서 배깅과 유사하나, 부스팅은 각 분류기가 독립적이지 않다.
- 이전 모델을 구축한 뒤 다음 모델에서, 이전 분류기가 잘못 분류한 데이터에 더 큰 가중치를 주어 붓스트랩 구성.
  - AdaBoost, Gradient Boost, XGBoost(GBM보다 빠르고 규제 포함), Light GBM(학습속도 개선, lear-wise)등이 있음.
  - 훈련 오차를 빠르게 줄일 수 있고 예측 성능 또한 배깅보다 뛰어나다 할 수 있음.
  - 부스팅은 이상값(가중치)에 민감함. 의사결정나무, 배깅, 랜덤포레스트는 민감 x

### 랜덤포레스트

- 배깅과 유사하나 배깅에 더 많은 무작위성을 주는 분석 기법, '여러 개의 약한 트리들의 선형결합'으로 최종 결과를 얻는 모델.
  - 분류의 경우 다수결, 회귀의 경우 평균or중앙값을 구하는 방법을 사용
- 배깅에서는 각 붓스트랩을 활용하여 트리를 구성할 때, 모든 마디의 불순도가 최소화되는 최적 분할을 실시(모든 변수)
- 그러나 랜덤포레스트에서는 각 마디에서 표본추출 과정이 한 번 더 반복되어 추출된 표본을 대상으로 최적 분할 실시.(각 마디마다 일부 변수)
  - 따라서, 랜덤포레스트는 큰 분산을 가진다는 의사결정나무의 단점을 보완하고, 모든 분류기들이 높은 비상관성을 가져 일반화 성능을 향상, 이상값에 민감하지 않다는 장점을 가짐.
- OOB(Out of Bag) Score

### 스태킹

- 일반적인 앙상블 기법은 각 분류기 간 결과를 보팅을 통해 최종 결과를 선정하지만, 스태킹은 분류기 간 결과를 다시 훈련용 데이터로 사용하여 최종 모형(메타모델)을 구축.
  - 높은 예측력 but 높은 복잡도, 오랜 시간, 결과 해석의 어려움

* 보팅,배깅,랜덤포레스트는 병렬처리 가능, 부스팅은 병렬처리 불가능

# 인공신경망 분석

- 은닉층 활성화함수

1. 스텝함수
2. 부호함수
3. 시그모이드(0~1)
4. tanh(-1~1): 시그모이드의 기울기 소실 문제 지연 but 해결x
5. ReLU: 기울기 소실 문제 해결

- 출력층 활성화함수

1. 시그모이드: 이중분류
2. 소프트맥스: 다중분류

- 손실함수

1. MSE : 회귀모델
2. 교차 엔트로피 : 분류모델

## 인공신경망 장점

1. 노이즈에 민감하게 반응하지 않음
2. 비선형적인 문제 해결에 유용
3. 스스로 가중치를 학습하여 다양하고 많은 데이터에 효과적

## 인공신경망 단점

1. 모형 복잡시 오랜 시간
2. 지역 극소점 문제
3. 추정한 가중치의 신뢰도가 낮다(가중치가 많고 복잡해)
4. 결과 해석의 어려움(은닉층)
5. 하이퍼파라미터 선정 어려움

## 나이브베이즈 분류

- 나이브(naive: 독립)
- 베이즈 정리: 사전확률과 사후확률 사이의 관계를 나타내는 정리
- 나이브베이즈 분류 모델은 베이즈 정리를 기반으로 한 지도학습 모델로, 스팸메일 필터링, 텍스트 분류 등에 사용.
- 모든 특징 변수가 서로 "동등하고 독립적이라는 가정"하에 분류 실행.
- 이진 분류 데이터, 범주 a, b가 될 확률을 구하고 더 큰 확률값이 나오는 범주에 데이터를 할당
- 과거의 경험 활용하는 '귀납적 추론 방법'

## k-NN 알고리즘

- 훈련이 따로 필요없는 Lazy Model
- 지도학습인 분류분석에 속함. 정답 라벨이 있는 데이터들 중 정답 라벨이 없는 데이터들을 어떻게 분류할 것인가?
  - 정답 라벨이 없는 새로운 데이터 입력 시 가장 가까이 있는 데이터의 정답 라벨을 확인하여 새로운 데이터의 정답 라벨을 결정하는 방식
- k값을 어떻게 정하는지가 관건, 보통 데이터 갯수 N의 제곱근 값 사용.
  - k=5 : 새로운 데이터와 가까운 데이터 5개를 보고 많은 쪽으로 판별

## 서포트 벡터 머신

- 선형/비선형 이진 분류, 회귀에서 사용 가능
- 초평면 이용, 높은 마진을 가져가는 방향으로 분류함.
  - 마진: 초평면과 서포트벡터(경계와 가장 가까운 샘플) 간 거리
- 커널함수: 저차원데이터를 고차원데이터로 변경하는 함수
- 하드마진: 마진 내 오류 비허용
- 소프트마진 : 마진 내 오류 어느정도 허용

# 분류 모형 성과 평가

## 혼동행렬 = 오분류표

- 정분류율(=정확도Accuracy)
- 오분류율(Error Rate)
- 민감도(=재현율)=TP Rate=Hit Rate
- 정밀도(Precision)
- 특이도(Specificity)
- 거짓 긍정률(FPR) = 1-특이도
- F1 Score
- F-Beta Score (정<>재)로 외우자
  - β>1 : 재현율에 큰 비중
  - β<1 : 정밀도에 큰 비중
  - β=1 : F1 Score와 동일

## ROC 커브

- x축: 거짓긍정률(1-특이도) 값을, y축은 민감도 값을 갖는 그래프로, 이진 분류 모형의 성능 평가 위해 사용
- ROC커브 아래의 면적을 나타내는 AUROC(Area Under ROC)값이 1에 가까울수록 모형의 성능이 우수, 0.5에 가까울수록 랜덤모델에 가까운 좋지 않은 모형임

## 이익도표(Lift Chart)

- 임의로 나눈 각 등급별로 반응검출률, 향상도 등의 정보를 산출하여 나타내는 도표
- 모델이 예측한 확률을 기준으로 데이터를 내림차순으로 정렬
- 상위 10%, 20%, ..., 100%까지 차례로 보며 정답(긍정 클래스)이 얼마나 포함되어 있는지 누적 계산
  - 일반적으로 0.5에서 cut off, 1.0이 가장 높은 기준
- 랜덤모델의 예측력 = 목표범주에 속한 데이터 수/전체 데이터 수
- 향상도 = 반응률/랜덤모델의 예측력
  - 빠르게 감소할수록 좋은 모델

## 향상도 곡선(Lift Curve)

- 이익도표를 시각화한 곡선
- "랜덤모델과 비교하여 해당 모델의 성과가 얼마나 향상되었는지" 구간별로 파악 위한 그래프
- 좋은 모델일수록 큰 값에서 시작하여 급격히 감소함.

## 중간 오답

- 역전파에 의한 가중치 수정 작업 중 가중치의 절댓값이 커져 과소적합이 발생하는 것을 포화문제라 한다.

1. 포화문제
   - 입력이 너무 크거나 작을 때 학습 거의 안 됨 → 과소적합
2. 학습률문제
   - 학습률(learning rate)이 너무 크면: 가중치가 너무 크게 조정되어 발산하거나 최적점을 지나침
   - 너무 작으면: 학습이 매우 느리거나 지역 최솟값에 갇힘

# 군집분석

## 관측치의 유사성 측정 척도 - 함수식 주고 함수명 맞추기 연습

1. 거리측도 - 변수가 연속형인 경우
   - 유클리디안 거리
   - 맨해튼 거리
   - 민코프스키 거리 : 맨해튼 거리 + 유클리디안 거리 -> p=1:맨해튼, p=2:유클리디안
   - 마할라노비스 거리 : 공분산 고려 S 한 표준화
   - 체비셰프 거리 : 가장 큰 축 방향 MAX|x-y| 거리만 고려
   - 표준화 거리 : 표준편차로 스케일 조정 후 거리계산
2. 유사성측도 - 변수가 범주형인 경우
   - 단순 일치 계수 : m/p (m: 객체 i,j가 같은 상태인 변수의 일치한 수, p: 변수의 총 개수)
   - 자카드 지수 : 두 집합 사이의 유사도를 측정하는 지표, 두 집합이 같으면 1, 완전히 다르면 0
   - 자카드 거리 : 자카드 지수를 거리화, 1-자카드 지수, 완전히 다르면 1, 완전히 동일하면 0의 값
   - 코사인 유사도 : 문서(텍스트)의 유사도를 측정하기 위한 지표, 크기가 아닌 방향성을 측정, 완전히 일치하면 1, 완전히 다르면 -1
   - 코사인 거리 : 코사인 유사도를 거리화하기 위해 1에서 코사인 유사도를 뺀 값
   - 순위 상관계수 : 순서척도인 두 데이터 사이의 거리 측정 위한 지표, 스피어만 상관계수 사용

## 군집분석의 평가 지표

- 실루엣 계수: 응집도와 분리도를 계산하며, 값이 1에 가까울수록 완벽하게 분리되었다고 판단, 0보다 작으면 잘못된 군집 분류 가능성
  - 실1 (실딱이 ㅋㅋ)

## 표준화와 정규화

- 표준화: 관측치들이 평균으로부터 얼마나 떨어져 있는지를 나타낼 때 사용, 평균은 0, 표준편차는 1 <-변수 간 분포 통일
- 정규화: 모든 데이터의 범위를 0과 1사이로 변환하여 분포를 조정하는 방법이다(min-max scaling)

## 계층적 군집분석

- k값을 미리 정할 필요 없음.
- 병합적 방법: 각 데이터를 가까운 데이터부터 순차적으로 병합해 나가는 방법
- 분할적 방법: 전체 데이터에서 군집을 순차적으로 분할하는 방법

## 군집간 거리 <- 방법 별 특성 숙지하기 (단 완 평 중 와)

1. 단일연결법(최단연결볍) : 가장 가까운 두 점 사이 거리로 군집 간 거리 정의, 이상치에 매우 민감
2. 완전연결법(최장연결법) : 가장 먼 두 점 사이 거리로 군집 간 거리 정의, 이상치에 다소 민감
   - 둥근 형태 군집 형성, hclust()의 디폴트 연결법임.
3. 평균연결법 : 모든 점 간 거리의 평균으로 군집 간 거리 정의
   - 단일,평균 연결법보다 이상치에 덜 민감!
4. 중심연결법 : 각 군집 중심점 간 거리로 군집 간 거리 정의
   - 계산량 적음 -> 한 번만 계산
5. 와드연결법 : 군집 내 오차(편차 제곱합) 최소화
   - 군집 내 분산을 최소화

## 비계층적 군집분석

- 구하고자 하는 군집의 수를 사전에 정의

1. k-means 군집: 군집의 수 k를 사전에 정한 뒤, 집단 내 동질성과 집단 간 이질성이 모두 높게 k개 군집으로 분할하는 알고리즘
   - 임의로 데이터 중에서 k개를 선택하는데 이때 k개 데이터를 seed라 한다.
   1. k를 설정하고 각각의 k를 설명할 변수를 임의로 설정하거나, 데이터 중에서 k개를 선택
   2. 각 데이터를 가장 가까운 seed로 할당
   3. 각 군집의 데이터들 사이의 평균값or중앙값 계산하여 새로운 seed 설정
   4. 새로운 seed중심으로 군집 재할당
   5. 각 군집의 중심이 변하지 않을때(모든 데이터가 이상적으로 군집화 될 때)까지 3,4번 과정을 반복한다.

### k-means 군집의 장점

1. 비교적 단순하고 빠르다
2. 다양한 데이터에 사용 가능하다

### k-means 군집의 단점

1. k의 설정 어려우며 결과 해석이 어렵다
2. 데이터의 변수들이 연속형 변수여야 한다[중요!] <- 비계층적 군집분석: 연속형 데이터여야 함 / 계층적 군집분석은 범주형 데이터에서도 가능!
3. 그리디 알고리즘으로 '안정은 보장하나 최적은 보장 x'
4. 이상값에 민감하다 (따라서 평균 대신 중앙값을 쓰기도 한다)

- 참고 k-medoids : 이상값에 민감한 k-means 군집 보완 위해 고안됨. 데이터 중 다른 데이터와의 거리가 최소인 데이터를 seed로 선택. 계산 수행 시간 큼

## DBSCAN(밀도기반군집분석)

- k 지정 필요 없음
- 밀도 기반 군집분석 - 개체 간 거리에 기반을 둔 다른 알고리즘과 다르게 개체들이 밀접한 정도에 기초해 군집 형성
- k-means와 달리 군집의 형태에 구애받지 않아, 분포가 기하학적이고 노이즈가 포함된 데이터셋에 대해서도 효과적으로 군집 형성 가능, 초기 군집 수 설정할 필요가 없음.
- 노이즈와 이상치에 강함

## 혼합 분포 군집(mixture distribution clustering)

- 이상치에 민감함
- 데이터들이 여러개의 확률분포로부터 추출되었다는 가정하에 같은 확률분포에서 추출된 데이터끼리 군집화하는 방법
- 각각의 확률분포의 모수와 가중치를 추정, 추정방법으로 'EM(기댓값 최대화)알고리즘'을 사용함.

### EM 알고리즘

- 확률모델의 최대가능도를 갖는 모수와 함께 그 확률모델의 가중치를 추정하고자 한다.
- 두 단계 E-step, M-step으로 구성

1. E-step
   - 파라미터(모수) 설정: 초기 파라미터(각 정규분포의 평균, 표준편차, 가중치) 값 임의로 설정
   - Z의 기댓값 계산 : 로그 가능도 함수 기댓값 계산
2. M-step
   - 새로운 파라미터(모수) 추정
   - 알고리즘 반복 및 종료

## 자기조직화 지도 SOM (=코호넨 맵)

- 인공신경망 기반 차원 축소와 군집화를 동시에 수행 가능
- 다차원 데이터를 축소해 저차원의 지도 생성, 데이터 가시화에 유용, 입력공간의 속성을 보존(실제로 유사한 데이터는 2차원 격자에서도 가깝게 표현됨)
- 은닉층 없이, 데이터 입력층과 노드 경쟁층으로 구성, 모든 데이터와 모든 노드는 완전연결(Fully-Connected)되어있음.
- 입력층에 가장 가까운 프로토타입 벡터를 'BMU(Best- Matching Unit)'이라 부름. 최종적으로 가장 가까운 하나의 노드에 도달(승자 노드)

1. 초기 학습률과 임의의 값의 가중치 행렬, 경쟁층의 노드 개수를 지정함
2. 입력 벡터(첫 번째 데이터)를 제시하고 가중치 행렬에 의하여 가장 가까운 노드에 나타냄
3. 입력 벡터에 대한 승자노드가 입력벡터를 더 잘 나타내도록 학습률을 사용하여 해당 가중치 재조정
4. 2단계로 돌아가 반복하여 모든 입력 벡터를 승자노드에 나타낸다.
   - 모든 입력벡터가 승자노드에 표현되는 과정을 1회의 iteration이라 한다.
5. 일정 iteration수에 도달할 때 까지 2번으로 돌아가 위 작업을 반복한다(반복 과정에서 승자노드가 변경될 수 있음!)

### SOM 장점

1. 역전파 알고리즘을 사용하지 않는 순전파 방식으로 속도가 매우 빠름
2. 저차원의 지도로 형상화되어 시각적 이해가 쉬움
3. 패턴 발견, 이미지 분석에서 성능 우수
4. 입력 데이터에 대한 속성을 그대로 보존함

### SOM 단점

1. 초기 학습률, 초기 가중치에 큰 영향을 받음
2. 경쟁층의 이상적인 노드 수 결정 어려움

## 군집분석 모형의 평가

1. 외부평가(정답이 있음)
   - 자카드 계수 평가 : 실제 클래스와 군집 결과 간의 겹침 정도
   - 분류 모형 평가 방법 응용(혼동행렬, ROC커브)
2. 내부평가(정답이 없음)
   - 단순 계산법
   - 군집 간 거리 계산
   - 실루엣 계수 : -1~1의 범위, 1에 가까울수록 군집이 잘 된 것
   - 엘보 메소드 : 왼만해지는 부분

# 연관분석

- 탐색적 기법의 일종으로 장바구니 분석으로도 불림, 비지도학습

## 연관분석의 측도

1. 지지도 : P(A∩B) = (A와 B가 동시에 포함된 거래 수) / (전체 거래 수)
2. 신뢰도 : 조건부확률, 신뢰도 (A->B) = P (B|A) , 신뢰도 (B->A) = P(A|B)로 서로 다름
3. 향상도 : 향상도 (A->B) = P(A∩B)/P(A)P(B), 향상도 (B->A) = P(A∩B)/P(B)P(A)로 서로 같음
   - 향상도 < 1 : 음의 상관관계, A가 구매될 때 B의 구매 확률 감소
   - 향상도 = 1 : 관계 없음
   - 향상도 > 1 : 양의 상관관계, A가 구매될 때 B의 구매 확률 증가

## 연관분석 알고리즘 - Apriori 알고리즘

- 지지도를 사용해 빈발 아이템 집합(후보빈발집합)을 판별하여 계산 복잡도를 감소시키는 알고리즘

1. 최소 지지도 설명
2. 최소 지지도보다 큰 지지도 갖는 단일 품목 선별
3. 2단계에서 찾은 단일 품목으로 2가지 생성되는 연관규칙 (A->B)중 최소 지지도 이상의 연관규칙 찾음
4. 위 과정 반복적 수행 하면서 3가지 이상의 품목에 대한 연관규칙 생성, 의미있는 결과 찾음

## 연관분석 알고리즘 - FP-Growth 알고리즘 (상향식)

- 데이터셋이 큰 경우 모든 아이템셋을 하나하나 검사하는 것이 비효율적이라는 문제점에서 탄생, 분할과 정복
- 지지도가 낮은 품목부터 지지도가 높은 품목 순으로 올라가며 빈도수가 높은 아이템 집합을 생성하는 '상향식' 알고리즘으로,
  - Apriori 알고리즘보다 속도가 빠르며, 연산 비용이 저렴

## 연관분석 장점

1. 결과가 단순하고 분명하다
2. 계산 간단
3. 목적변수가 없어 데이터 탐색을 위해 사용 가능

## 연관분석 단점

1. 품목 세분화에 어려움 존재
2. 품목 수의 증가는 계산량의 기하급수적 증가를 불러옴
3. 거래가 발생하지 않은 품목은 분석이 불가능

## 추가: 순차패턴 (시간의 흐름을 고려한 연관 분석)

- 연관분석에 시간 개념이 추가되어 '프린터를 구매한 고객은 일정 시간 내 종이를 구매한다'와 같은 규칙을 찾는 분석기법
- 연관분석은 장바구니를 언제 누가 들고있었는지에 대한 정보는 필요 없지만, 순차패턴은 장바구니를 누가 언제 들고있었는지에 대한 고객과 시간의 정보가 함께 필요함

## 3과목 기출 오답 및 정리 feat. 미어캣 책 쓰레기네..

1. 회귀분석에서는 설명변수의 동일수준의 다른 관측치에 비해 종속변수의 상이한 값을 이상치로 판정한다.
   - => 설명변수(독립변수)의 값이 같은데, 종속변수(결과값)만 유난히 다른 관측치를 이상치라고 본다.
2. LGBM(light gradient boosting machine)은 leaf-wise 방법을 사용함
3. Lasso 회귀(L1 규제)

- 람다값으로 panalty 정도 조정
- 회귀계수의 절댓값 클 수록 강한 panalty 부여
- 회귀계수 중 일부를 0으로 만들어 자동 변수 선택 효과 발생

4. 지수분포는 연속형 확률 분포
5. 주성분분석

- 상관관계가 있는 변수들을 선형결합해 상관관계가 없는 변수로 축약하는 방법임
- 공분산 행렬을 사용하는 경우 변수들의 측정 단위에 민감함

7. Box plot으로 관측치 개수를 알 수는 없음
8. 지지도, 신뢰도, 향상도 문제에서!!!

- 지지도, 신뢰도가 높은 것이, 두 품목을 같이 배치할 때 잘 팔리는 것을 보장하는 것은 아님.(양의상관관계 보장 x)

9. 부호 검정: 짝지어진 두 개의 관찰치의 크고 작음에 대한 가설을 검증하는 방법

- AdaBoost, LGBM, XGBoost(GBM보다 빠르고 규제 포함) 등 부스팅 알고리즘의 간단한 특징 알아두자
- 비모수적 방법인 크루스칼 왈리스, 윌콕슨 순위합, 부호 순위 검정, 만-휘트니 검정 등 특징 알아두기
  1.  윌콕슨 순위합, 만-휘트니 : 독립된 두 집단(독립표본t검정)
  2.  윌콕슨 부호 순위, 부호검정 : 단일, 대응표본t검정 대안
  3.  크루스칼-왈리스 : 3개이상집단(ANOVA대안)
  4.  부호검정:방향만 사용
  5.  런검정:연속성검정
- BIC, AIC 그래프 보는 법 익히기
- Box Plot 보는 법 익히기
- 랜덤포레스트는 기존의 배깅 방법에 특징 배깅을 더한 방법이다

  - 배깅: 데이터 샘플을 여러 번 복원추출 → 여러 트리 생성
  - 랜덤포레스트: 데이터 샘플도 무작위, 특징도 무작위로 선택

- R에서 rpart패키지를 활용하여 의사결정나무를 수행할 수 있다 .
- SOM <- 군집분석의 일종

- prcomp = princomp(test, cor=T)?? 뭔 소리야..

## 3과목 예상문제

4. 군집분석은 이상값에 민감함.
5. data=c(1,2,3,4)일때 data+5
   - R에서 벡터와 상수의 사칙연산은 백터의 모든 성분과 연산을 한다. 1+5,2+5,3+5,4+5 -> 6,7,8,9가 됨
6. 연관분석의 측도들

- 신뢰도가 커도, 향상도가 1보다 작으면, 같이 진열될 때 구매 확률의 감소가 일어난다. 신뢰도가 커도 향상도<1일 수 있음을 주의!

13. 주성분분석에서 comp와 var 이 빈공간이어도, -0.1~0.1 사이의 값으로 변수가 주성분에 미치는 영향이 거의 없다고 판단할 뿐이지, 영향이 전혀없다(0이다)라고 확정지을 순 없다.
14. 분산분석은 두 개 이상의 집단의 평균을 비교하기 위한 가설검정이다. (두 개 일때는 t검정과 분산분석 결과가 거의 동일)

- 세 개 이상의 집단에 대한 분산분석으로 귀무가설을 기각할 때, 어떠한 집단끼리 평균이 다른지 알 수 없어, 사후 검정이 필요하다.

19. data = 'banana' -> data[3:4] 수행하면, NA, NA 출력함. data는 banana하나가 있는 벡터(리스트 느낌), 인덱스 3부터 인덱스 4까지의 값 없음.
20. 비모수적 방법은 통계적 추론에 있어 평균과 분산 사용하지 않음
21. seq함수. by:간격을 의미, length.out:수열의 개수 의미.
22. sd함수는 주어진 벡터의 모표준편차가 아닌 표본표준편차를 계산함.
23. type=''

- type='n'은 none으로 산점도에 아무것도 나타내지 않음
- type='p'는 기본값(point)으로 산점도의 데이터를 점으로 나타냄
- type='l'은 line으로 산점도의 데이터를 선으로 이어서 나타냄
- type='b'는 bi로 점과 선을 모두 사용하여 나타냄

30. 단순회귀분석에서 분산분석표의 잔차 자유도는 n-2 (표본수 - bias 1개- 독립변수x 1개)
31. Q1: 25%, Q3: 75%
32. x와 x2를 사용 -> 다항회귀모형?
33. hclust의 method기본값은 complete, 최장연결법이다. method='single'에서 single을 생략하면, complete를 수행한다.

- 맨해튼 거리는 다른말로 '시가 거리'라고 한다.

48. 두 개 집단(수도권, 비수도권)의 내부 구성의 비율에 차이가 없는지 있는지 비교하기 위한 가설검정에 카이제곱 분포를 활용한 동질성 검정을 수행.

- 카이제곱(동질성검정): 범주×범주 ⇒ 분포(비율) 차이가 있는가?
- F검정: 그룹×연속 ⇒ 평균 차이가 있는가?

50. Mixture Of Normal Distribution은 혼합 분포 군집이다. Multidimensional Scaling(다차원 척도법:MDS)는 통계 분석으로 시각화 기법의 한 종류.
51. R은 프리웨어보다 더 개방적인 오픈소스이다. 프리웨어(프리 소프트웨어)는 무료 사용은 가능하나 코드에 접근할 수 없다.
52. rbind: 두 데이터를 행으로써 병합, cbind: 두 데이터를 열로써 병합
53. readxl: xml이 아니라, 엑셀 csv파일 읽기 위한 라이브러리, party: 의사결정나무를 위한 ctree함수를 지원함
54. step(lm(data = cars, y ~ 1), scope = list(upper = ~ x1+x2+x3), direction='foward')
55. 보팅이란 여러 트리로부터 얻은 결과값을 다수결 방식에 의하여 최종 결과를 선택하는 방법.

- 가장 강력한 트리로부터 최종결과를 얻는것~ <- 이건틀림

76. 체비셰프 거리 구하기

- A와 B의 x,y,z 성분 간 차이의 절댓값 중 가장 큰 것을 고르고, 체비셰프 거리가 가장 작은 것 끼리 묶어 군집 수행.

84. R에서 3차원 이상의 구조로 모든 데이터가 같은 타입을 갖는 것 -> 배열
85. 분류 분석의 모형을 평가하기 위한 방법 중 하나로, 랜덤 모델에 비하여 모델의 성과가 얼마나 향상되었는지를 등급별로 파악하는 그래프

- 향상도 곡선에 대한 설명임
  - 편차 제곱합 그래프(Elbow Plot 등): 군집의 적정 k를 찾는 시각화 방법
  - 자기상관그래프(ACF): 시계열 데이터에서 시차별 자기상관 계수를 막대 그래프로 표시

## 1회 모의고사

- DBSCAN : 밀도기반군집
- 공분산 <- 범위 음의 무한대 ~ 양의 무한대
- 다중대치법 <- 대치 분석 결합 순서

- 군집<- 빅분기 필기 책이 더 잘 나온 것 같음
- 각 분석 방법들의 장점&단점 꼭꼭 숙지하자.

2. 가트너가 제시한 데이터 사이언티스트의 필요 역량: 데이터 관리, '분석' 모델링, 비즈니스 분석, 소프트 스킬(의사소통, 협력, 리더십, 창의력, 규율, 열정) - 가트너 데모비소(의협리창규열)
3. ISP는 정보기술과 정보시스템을 조직의 전략적 목표 달성에 효과적으로 활용하기 위한 중장기 계획 수립 과정이다.

- 단순한 시스템 구축이나 유지보수가 아니라, 조직 전략과 정보기술을 연계하는 포괄적인 계획 수립 과정이다.

21. 등분산 검정은 두 집단의 분산이 동일한지 여부를 판별하기 위해 수행하는 검정으로, F 분포를 사용.

- F분포는 등분산 검정 외에도 분산분석(ANOVA)에서 활용된다

24. 귀무가설이 참인데도 기각할 확률 : 제 1종오류(유의수준)

- 귀무가설이 참일 때, 지금처럼 극단적인 데이터가 나올 확률 : 유의확률(p-value)

43. k-means 군집

- k-평균 군집에선 최단연결법 사용하지 않으며, seed값은 무작위로 선택하거나 사용자가 지정할 수 있다.
- k-평균 군집은 이상값에 민감하며(평균이니까.), k-medoids 알고리즘은 중앙값을 사용하여 이를 보완한다
- 초기 seed선택에 따라 최종 군집 결과가 달라질 수 있다.
- 앞서 사전에 주어진 목적이 없어 결과 해석이 어렵다 (사전에 주어진 목적 = label = 지도학습)

44. 다중회귀분석

- 결정계수와 수정된 결정계수의 차이가 거의 없다해서 이것이 변수 독립성을 의미하진 않음. 다중공선성 여부는 VIF 등을 통해 추가 확인 해야함.

45. 단순회귀분석

- 최소제곱법은 실제값과 추정값의 차이(잔차)의 제곱합을 최소화하여 최적의 '회귀계수'와 '회귀상수'를 찾는(모수추정) 방법임.

46. 다중 대치법

- 대치: 결측값을 여러 번 대치하여 여러 개의 완전한 데이터 셋을 생성 (결측값 들어가면 대치)
- 분석: 각 대치된 데이터셋에 대해 통계 분석 수행 (수행 들어가면 분석)
- 결합: 여러 분석 결과를 종합하여 최종 추정치와 표준오차를 계산 (결과 들어가면 결합)
  - 단순 대치법보다 더 정확한 결과를 제공, 결측 매커니즘의 불확실성을 반영할 수 있음
